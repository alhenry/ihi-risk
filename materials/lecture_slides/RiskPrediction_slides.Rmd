---
title: "Risk Prediction"
subtitle: "Advanced Statistical Analysis"
author: "Albert Henry\nbased on slides by L. Palla, D. Prieto, and E. Williamson"
institute: "Institute of Health Informatics"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center, middle, inverse
background-color: #1f2430
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)

accent <- "#0f4c81"
white <- "#fafafa"
black <- "#101010"
# accent2 <- "#A4DBE8"
mono_accent(
  base_color = accent,
  # secondary_color = accent2,
  black_color = black,
  white_color = white,
  header_font_google = google_font("Raleway"),
  text_font_google   = google_font("Lato"),
  code_font_google   = google_font("Inconsolata"),
  table_row_even_background_color = "#f0f0f0",
  header_font_weight = "bold",
  text_font_size = "24px",
  link_color = "deeppink"
)

extra_css <- list(
  ".accent" = list(color = accent),
  ".shadow" = list("text-shadow" = "0 0 10px #000"),
  ".blue2" = list(color = "#36BDF2"),
  ".blue3" = list(color = "#00F2F5"),
  ".gray" = list(color = "lightgray"),
  ".yellow" = list(color = "#FFCC66"),
  ".red"   = list(color = "firebrick", opaciy = "1"),
  ".blue" = list(color = "steelblue", opaciy = "1"),
  ".orange" = list(color = "orange", opaciy = "1"),
  ".green" = list(color = "seagreen", opacity = "1"),
  ".violet" = list(color = "darkorchid", opaciy = "1"),
  ".pink" = list(color = "deeppink", opaciy = "1"),
  ".large" = list("font-size" = "120%"),
  ".big" = list("font-size" = "400%"),
  ".small" = list("font-size" = "90%"),
  ".smaller" = list("font-size" = "75%"),
  ".translucent" = list("opacity" = "0.4"),
  ".full-width" = list(
    display = "flex",
    width   = "100%",
    flex    = "1 1 auto"
  )
)

write_extra_css(extra_css)
```


# .yellow[Risk Prediction]

### .yellow[Advanced Statistical Analysis]

### .yellow[Albert Henry]

#### based on slides by L. Palla, D. Prieto, and E. Williamson


<br/>

### .yellow[UCL Institute of Health Informatics]

these slides are available online at:

.accent[https://ihi-risk-teaching.netlify.app/]

---
# Learning objectives

- Describe the difference between __deterministic__ and __probabilistic__ methods for prediction

- Create __2x2 contigency table__

- Explain how __ROC curves__ are calculated and how they relate to __sensitivity__ and __specificity__

- Understand key measures to evaluate __model performance__ 

---
# The problem

.small[
Consider individuals in which a certain __binary event Y__ might
happen (0=no, 1=yes) by a certain point in time. 
We also have a set of other variables for each individual.

Some examples:
- A group of patients about to have a surgery: Some will __develop complications after the surgery__ and some will not. We have data on their age, sex , severity of disease, comorbidities etc.

- A group of pregnant women: some will __have a child with a malformation__ , some will not. We have data on the mothers â€™ age , medications, diagnostics, life style, etc.

In all these cases we do not know for sure which individuals will get the event.
]

---
# What we want to do
Ideally we would like to predict who is __going to develop the event__. This can be done in two ways:
- __Deterministic:__

  **Classify** each individual in one of the two possible outcomes: develop the event or not.
  This is often used in __(Supervised) Machine Learning__
- __Probabilistic:__
  
  Assign each individual a **probability** of developing the event. ( This is often used in __Biostatistics__ and is known as **Risk Prediction**
  
Both strategies use data we collected for each individual (age, sex, diagnoses, etc.)

---
## Example
__Who will develop myocardial infarction in the next 5 years?__

Consider we have the following data

```{r, echo=FALSE, message=F}
library(tidyverse)
library(knitr)
library(kableExtra)

options(knitr.kable.NA = '')
# kable <- function(df, ...){
#   kable(df, align = "c", ...) %>% 
#     kable_styling(bootstrap_options = c("hover", "striped"),
#                 full_width = T, font_size = 20)
# }

df <- tibble(id = 1:10,
             age = c(35, 35, 55, 55, 65, 65, 75, 75, 85, 85),
             sex = c("F", "M", "F", "M", "F", "M", "M", "F", "F", "M") %>%
               factor(levels = c("F", "M")),
             diabet = c("Yes", "No", "No", "Yes", "No", "Yes", "Yes", "No", "Yes", "No"),
             SBP = c(145, 130, 115, 170, 135, 140, 160, 130, 130, 160),
             Classify = c("No", "No", "No", "Yes", "No", "Yes", "Yes", "No", "Yes", "Yes"),
             Predict = c(0.15, 0.05, 0.10, 0.55, 0.30, 0.52, 0.60, 0.40, 0.55, 0.60),
             Observed = c("No", "No", "No", "No", "Yes", "No", "Yes", "No", "Yes", "Yes")) %>% 
  mutate_at(vars(diabet, Classify, Observed),
            factor, levels = c("Yes", "No"))


df %>%
  mutate(Classify = NA, Predict = NA, Observed = NA) %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("hover", "striped"),
                full_width = T, font_size = 20) %>% 
  row_spec(0, background = "#bbbbbb")
  # row_spec(1, bold = T, color = "white", background = "#") %>% 

```

---
## Classify deterministically

Use previous algorithm:

`C(age, sex, diabet, SBP) = 0 or 1`

```{r, echo=FALSE}
df %>%
  mutate(Predict = NA, Observed = NA) %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("hover", "striped"),
                full_width = T, font_size = 20) %>% 
  row_spec(0, background = "#bbbbbb") %>% 
  column_spec(6, color = accent, bold = T)
  # row_spec(1, bold = T, color = "white", background = "#") %>% 

```
---
## Predict probabilistically

Use previous algorithm:

`P(age, sex, diabet, SBP) = [0, 1]`

```{r, echo=FALSE}
df %>%
  mutate(Classify = NA, Observed = NA) %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("hover", "striped"),
                full_width = T, font_size = 20) %>% 
  row_spec(0, background = "#bbbbbb") %>% 
  column_spec(7, color = accent, bold = T)
  # row_spec(1, bold = T, color = "white", background = "#") %>% 

```
---
## Observe
... after 5 years follow-up

Compare predictions with observed events

```{r, echo=FALSE}
df %>%
  kable(align = "c") %>%
  kable_styling(bootstrap_options = c("hover", "striped"),
                full_width = T, font_size = 20) %>% 
  row_spec(0, background = "#bbbbbb") %>% 
  column_spec(6:8, color = accent, bold = T)
  # row_spec(1, bold = T, color = "white", background = "#") %>% 

```

---
## Model validation

.large[
* Our models have predicted some individuals well but not others

* We want to have a general measure of how good or bad our algorithms are by
__comparing the predictions with the actual observed values__.

* This will be done differently for a classification algorithm than for a prediction
algorithm.
]

---
class: center

### 2x2 Contigency table /  Confusion matrix

```{r, echo=F, out.width="600px"}
knitr::include_graphics("figure/ConfusionMatrix_meme.png")
```

---
## Sensitivity

* a.k.a __True Positive Rate__, __Recall__

* Probability of __correctly predicting the event__

* Given you are someone who will __have the event__, what is the probability of a __positive classification__?

<br/>

.pull-left[
```{r, echo=F}
conf_matrix <- tibble(
  Predicted = c("+", "-"),
  `+` = c("TP", "FN"),
  `-` = c("FP", "TN")
)

conf_matrix %>% 
  kable(align = "c") %>% 
  kable_styling %>% 
  add_header_above(c("", "Observed" = 2)) %>%
  column_spec(1, bold = T, background = "#f0f0f0") %>%  
  column_spec(2:3, background = white, width = "4cm", color = accent) %>%
  column_spec(2, background = "#98FB98")
```
]

.pull-right[

.small[
$${Sensitivity}=\frac{\sum{True~Positive}}{\sum{Observed~Positive}}$$
$${Sensitivity}=\frac{TP}{TP + FN}$$
]
]

---
## Specificity

* a.k.a __True Negative Rate__

* Probability of __correctly predicting the__ ___non-___
__event__

* Given you are someone who will ___not___ __have the event__, what is the probability of a __negative classification__?

<br/>

.pull-left[
```{r, echo=F}

conf_matrix %>% 
  kable(align = "c") %>% 
  kable_styling %>% 
  add_header_above(c("", "Observed" = 2)) %>%
  column_spec(1, bold = T, background = "#f0f0f0") %>%  
  column_spec(2:3, background = white, color = accent, width = "4cm") %>% 
  column_spec(3, background = "#98FB98")
```
]

.pull-right[

.small[
$${Specificity}=\frac{\sum{True~Negative}}{\sum{Observed~Negative}}$$
$${Specificity}=\frac{TN}{TN + FP}$$
]
]

---
## Positive predictive value (PPV)

* Probability of __having the event__ given a __positive classification__

* __Positively correlated__ with __disease prevalence__

<br/><br/><br/>

.pull-left[
```{r, echo=F}

conf_matrix %>% 
  kable(align = "c") %>% 
  kable_styling %>% 
  add_header_above(c("", "Observed" = 2)) %>%
  column_spec(2:3, background = white, color = accent, width = "4cm") %>%
  row_spec(1, background = "#98FB98") %>% 
  column_spec(1, bold = T, background = "#f0f0f0") 
```
]

.pull-right[

.small[
$${PPV}=\frac{\sum{True~Positive}}{\sum{Predicted~Positive}}$$
$${PPV}=\frac{TP}{TP + FP}$$
]
]

---
## Negative predictive value (NPV)

* Probability of ___not___ __having the event__ given a __negative classification__

* __Negatively correlated__ with __disease prevalence__

<br/><br/><br/>
.pull-left[
```{r, echo=F}

conf_matrix %>% 
  kable(align = "c") %>% 
  kable_styling %>% 
  add_header_above(c("", "Observed" = 2)) %>%
  column_spec(2:3, background = white, color = accent, width = "4cm") %>%
  row_spec(2, background = "#98FB98") %>% 
  column_spec(1, bold = T, background = "#f0f0f0") 
```
]

.pull-right[

.small[
$${NPV}=\frac{\sum{True~Negative}}{\sum{Predicted~Negative}}$$
$${NPV}=\frac{TN}{TN + FN}$$
]
]
---
### Comparing _classifications_ with observations

```{r, echo=F}

tab <- with(df, table(Classify, Observed))
disp_tab <- function(tab){
  df.conf <- tibble(
  Predicted = c("+", "-"),
  `+` = tab[,1],
  `-` = tab[,2]
  ) %>% 
  add_row(Predicted = "Total", `+` = sum(.$`+`), `-` = sum(.$`-`)) %>% 
  rowwise %>% 
  mutate(Total = sum(`+`, `-`))

df.conf %>% 
  kable(align = "c") %>% 
  kable_styling %>% 
  add_header_above(c("", "Observed" = 2,"")) %>%
  column_spec(2:4, background = white, width = "4cm") %>% 
  column_spec(4, background = "#f0f0f0", color = accent, bold = T) %>% 
  row_spec(3, background = "#f0f0f0", color = accent, bold = T) %>%
  column_spec(1, bold = T, background = "#f0f0f0", color = black)
}

list_est <- function(tab){
  TP <- tab[1,1]; FP <- tab[1,2]
  FN <- tab[2,1]; TN <- tab[2,2]
  sens <- TP / (TP + FN)
  spec <- TN / (TN + FP)
  PPV <- TP / (TP + FP)
  NPV <- TN / (TN + FN)
  
  return(list(TP = TP, FP = FP,
              FN = FN, TN = TN,
              sens = sens, spec = spec,
              PPV = PPV, NPV = NPV))
}

disp_calc <- function(l){
  sens <- with(l, paste0(TP, "/", TP+FN, " = ", sens %>% round(2)))
  spec <- with(l, paste0(TN, "/", TN+FP, " = ", spec %>% round(2)))
  PPV  <- with(l, paste0(TP, "/", TP+FP, " = ", PPV  %>% round(2)))
  NPV  <- with(l, paste0(TN, "/", TN+FN, " = ", NPV  %>% round(2)))

  return(list(sens = sens, spec = spec,
              PPV = PPV, NPV = NPV))
}

disp_tab(tab)

```


.large[
.pull-left[
Sensitivity __= ?__

Specificity __= ?__
]

.pull-right[
PPV __= ?__

NPV __= ?__
]
]

---
### Comparing _classifications_ with observations

```{r, echo=F}

disp_tab(tab)

est <- disp_calc(list_est(tab))

```


.large[
.pull-left[
Sensitivity __= `r est$sens`__

Specificity __= `r est$spec`__
]

.pull-right[
PPV __= `r est$PPV`__

NPV __= `r est$NPV`__
]
]
---
### Comparing _predictions_ with observations

.small[

Order by risk and choose a __cut-off point__ to classify as "Yes", e.g.

__"Yes" if probability (P) > 0.1__

```{r, echo=F}
library(rlang)
cut_df <- function(df, threshold){
  df %>%
    mutate(Pred_class =  ifelse(Predict > threshold, "Yes", "No") %>%
           factor(levels = c("Yes", "No")))
}

disp_df <- function(df, threshold){
  col <- paste0("Prob >", quo_name(threshold))
  df <- df %>%
    mutate(Pred_class = cell_spec(Pred_class, "html",color = ifelse(Pred_class == "Yes", "green", "red")))
  
  df %>%
    select(id, Predict, Observed, !!col := Pred_class) %>%
    arrange(Predict) %>%
    kable(escape = F) %>%
    kable_styling()
}

df2 <- cut_df(df, 0.1)

disp_df(df2, 0.1)

```

]

---
### Cut-off point: _Yes_ if P > 0.1

```{r, echo=F}

tab <- with(df2, table(Pred_class, Observed))
est <- disp_calc(list_est(tab))

disp_tab(tab)

```


.large[
.pull-left[
Sensitivity __= `r est$sens`__

Specificity __= `r est$spec`__
]

.pull-right[
PPV __= `r est$PPV`__

NPV __= `r est$NPV`__
]

]

<br/><br/>

Higher sensitivity, lower specificity than the classification algorithm
---
### Cut-off point: _Yes_ if P > 0.4

.small[

```{r, echo=F}
df3 <- cut_df(df, 0.4)

disp_df(df3, 0.4)

```

]

---
### Cut-off point: _Yes_ if P > 0.4

```{r, echo=F}
tab <- with(df3, table(Pred_class, Observed))
est <- disp_calc(list_est(tab))

disp_tab(tab)

```


.large[
.pull-left[
Sensitivity __= `r est$sens`__

Specificity __= `r est$spec`__
]

.pull-right[
PPV __= `r est$PPV`__

NPV __= `r est$NPV`__
]

]

<br/><br/>

Same contigency table as the classification algorithm

---
### Cut-off point: _Yes_ if P > 0.55

.small[

```{r, echo=F}
df4 <- cut_df(df, 0.55)

disp_df(df4, 0.55)

```

]
---
### Cut-off point: _Yes_ if P > 0.55

```{r, echo=F}
tab <- with(df4, table(Pred_class, Observed))
est <- disp_calc(list_est(tab))

disp_tab(tab)

```


.large[
.pull-left[
Sensitivity __= `r est$sens`__

Specificity __= `r est$spec`__
]

.pull-right[
PPV __= `r est$PPV`__

NPV __= `r est$NPV`__
]

]

<br/><br/>

Lower sensitivity, higher specificity than the classification algorithm
---
##Â All cut-off points

.small[
If we repeat this process for each probability value,
we can obtain a list of sensitivity and specificity values

```{r, echo=F}
calc_sens <- function(threshold){
  dfx <- cut_df(df, threshold)
  tab <- with(dfx, table(Pred_class, Observed))
  tab[1,1] / sum(tab[,1])
}

calc_spec <- function(threshold){
  dfx <- cut_df(df, threshold)
  tab <- with(dfx, table(Pred_class, Observed))
  tab[2,2] / sum(tab[,2])
}

df_all <- df %>%
  select(id, Predict, Observed) %>% 
  arrange(Predict) %>% 
  rowwise %>% 
  mutate(`Cut-off` = paste0("P >", Predict),
         Sensitivity = calc_sens(Predict) %>% round(2),
         Specificity = calc_spec(Predict) %>% round(2))

df_all %>%
  kable() %>% 
  kable_styling()
```

]
---
### .small[Receiver Operating Characterictic (ROC) Curve]


.small[A curve linking all the sensitivity against the specificity values]

.left-column[
```{r, echo = F}
df_all %>% 
  select(Sens = Sensitivity, Spec = Specificity) %>% 
  kable() %>% 
  kable_styling()
```
]

.right-column[
```{r, echo=F, fig.dim=c(6,4), out.width="100%"}
df.plot <- df_all %>% ungroup %>% 
  add_row(Specificity = 0, Sensitivity = 1)

plot <- ggplot(df.plot, aes(x = Specificity, y = Sensitivity)) +
  geom_segment(aes(xend = Specificity, x = Specificity,
                   yend = Sensitivity),
               y = 0, linetype = "dashed", color = "grey44") +
  geom_segment(aes(yend = Sensitivity, y = Sensitivity,
                   xend = Specificity),
               x = 0, linetype = "dashed", color = "grey44") +
  geom_point(size = 2, color = accent) +
  geom_line(color = accent) +
  scale_x_continuous(breaks = df.plot$Specificity) +
  scale_y_continuous(breaks = df.plot$Sensitivity) +
  theme_classic()

plot
```
]

---
### .small[Area Under the [ROC] Curve (AUC / AUROC)]

What is the AUC?


```{r, echo=F, fig.dim=c(6,4), out.width="100%"}
plot
```

---
### .small[Area Under the [ROC] Curve (AUC / AUROC)]

What is the AUC?

Sample calculation with [yardstick package](https://github.com/tidymodels/yardstick) in R

```{r, message=F}
# df = the dataset shown in previous slides
roc <- yardstick::roc_auc(df, truth = Observed, Predict)
roc
```

__AUC = `r roc$.estimate`__

---
### .small[AUC and the distributions of predictors in the two outcome groups at different cut-off values]

![](https://github.com/dariyasydykova/open_projects/blob/master/ROC_animation/animations/cutoff.gif?raw=true)

.footnote[by Dariya Sydykova ([follow link for more info](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation))]


---
### AUC as a measure of model discrimination

![](https://github.com/dariyasydykova/open_projects/blob/master/ROC_animation/animations/ROC.gif?raw=true)

.footnote[by Dariya Sydykova ([follow link for more info](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation))]
---
### What does AUC tell us?

* AUC can be interpreted as the __probability that an observed â€œyes â€ was assigned a higher probability than an observed â€œnoâ€__

* If the model was __useless__ (same as assigning probabilities â€œat randomâ€),
then an observed â€œyesâ€ would have only 50% chances of having higher predicted risk than an observed â€œnoâ€ (__AUC = 0.5__)

* __AUC = 1__ for a model that would give higher predictions to all the
observed â€œyesâ€ than to all the observed â€œno â€ (__perfect separation__ between â€œyesâ€ and â€œnoâ€).

* Any real world model will have __AUC between 0.5 and 1__. Closer to 1 indicates better performance in separating "yes" and "no".

* For binary classification, AUC is equal to __concordance (C) statistic__

---
### How do we come up with predictions?

* We propose a statistical model for the __probability of the event happening__
$P\left(Y_{i}=1\right)$ depending on the other variables 

* For example a __logistic model__:

$$\log \left(\frac{P\left(Y_{i}=1\right)}{1-P\left(Y_{i}=1\right)}\right)=\beta_{0}+\beta_{1} X_{i}+\beta_{2} Z_{i}+\cdots \tag{1}$$

* We need a __training set__ where we can observe all the variables $Y_{i}, X_{i}, Z_{i}, \ldots$
to estimate the __coefficients__ $\beta_{0}, \beta_{1}, \beta_{2}, \dots$

* Once we have the coefficients that best fit the data we can __calculate the predicted risk for each individual__ $i$

$$\widehat{P}\left(Y_{i}=1\right)=\frac{e^{\widehat{\beta}_{0}+\widehat{\beta}_{1} X_{i}+\widehat{\beta}_{2} Z_{i}+\cdots}}{1+e^{\widehat{\beta}_{0}+\hat{\beta}_{1} X_{i}+\hat{\beta}_{2} Z_{i}+\cdots}} \tag{2}$$

---
.small[
### Model validation

#### Internal validation

* The validity of claims for the underlying __population that the data originated from__ __(reproducibility)__

* __Split sample validation__: split dataset randomly into __training__ (for model development) and __test__ set (for validation)
<br/>ðŸ‘‰ __.red[today's practical]__

* Other methods: __cross validation__ and __bootstrap__ resampling

#### External validation
* Generalizability of claims to __â€˜plausibly relatedâ€™ populations__ not included in the initial study population __(transportability)__

* e.g. __temporal__ or __geographical__ validation

.footnote[[Steyerberg EW, Vergouwe Y (2014)](https://doi.org/10.1093/eurheartj/ehu207)]
]

---
### A larger example with 2000 individuals

We will use the variables `Age, Sex, SBP, and BMI` to predict if the person will be `Dead = 0` or `Alive = 1` in 5 years time

```{r, echo=F, out.height="400px"}
knitr::include_graphics("figure/stata_data.png")
```
---
### Model _M1_: Logistic regression

Stata command:

`logistic dead age sex sbp bmi`

```{r, echo=F, out.width="100%"}
knitr::include_graphics("figure/stata_logReg.png")
```

Note that BMI is not statistically significant (`P = 0.185`) but let's stay with this model for now

---
### Make predictions from Model _M1_

Create variable: __logit of the probability of death__ - __equation (1)__

`predict m1lp, xb`

Create variable: __predicted probability of death__ - __equation (2)__

`predict m1lp`

```{r, echo=F, out.width="100%"}
knitr::include_graphics("figure/stata_logReg_predict.png")
```

---
#### Predicted probability of death in __`dead`__ and __`alive`__ group

![:scale 90%](figure/stata_boxPlot_1.png)


---
## Model calibration

Evaluate __goodness of fit__ with __Hosmer-Lemeshow test__

`estat gof, group(10) table`

![](figure/stata_gof.png)

---
## Model calibration

__Goodness of fit__: Observed and expected events by deciles of risk

.center[![:scale 60%](figure/stata_gof_decile.png)]

--
.smaller[__.red[Note:]__ Hosmer-Lemeshow test can not tell the direction of micalibration and relies on arbitrary grouping (see [Steyerberg EW, Vergouwe Y (2014)](https://doi.org/10.1093/eurheartj/ehu207) for discussion on the limitations and suggested alternatives)]

---
### Contingency table: cut-off $P(Y = 1) \geq 0.3$

`estat classification, cutoff(0.3)`

```{r, echo=F, out.height="100%"}
knitr::include_graphics("figure/stata_classify.png")
```

---
### ROC curve from model _M1_

.center[
![:scale 60%](figure/stata_roc_1.png)

__`AUC = 0.78`__
]

There is a 78% probability that a person that actually
dies gets a higher predicted risk by the model than a person
that did not die by the end of the follow up
---
.small[
### Sensitivity and Specificity for model _M1_ <br/> by cut-off value

.center[![:scale 65%](figure/stata_sensSpec_1.png)]

Plotting __sensitivity and specificity against cut-off value__ can help to
select the most appropriate cut-off

In practice, this trade-off often needs to be decided on a __case-by-case__ basis

]

---
## For discussion

Consider the following scenarios:

* Screening for diabetes & pre-diabetes

* Diagnosing coeliac disease

* Screening suspect for coronavirus infection COVID-2019

<br/>
Which model would you choose:

1. High-sensitivity, low-specificity

2. Low-sensitivity, high-specificity

---
#### Key measures to evaluate __model performance__

__Calibration__
* The agreement between the predicted & observed outcomes
* For a group of patients with 10% predicted risk, do 10% experience the event?
* e.g. Goodness of fit test

__Discrimination__
* The ability of the model to distinguish between "event" and "non-event"
* e.g. AUC / C statistic

__Clinical usefulness__
* Does the model provide accurate predictions at the patient level that can be used to guide clinical decision making?
* e.g. [Decision curve analysis](http://www.decisioncurveanalysis.org/)

---
## Summary

* Prediction modelling can be broadly categorised into __deterministic__ and __probabilistic__ methods

* __2 x 2 contigency table / confusion matrix__ is useful as a first step to evaluate model performance

* __AUC__ is a useful measure of the model __discrimination__

* Comparing observed and predicted risks is useful for model __calibration__

* Assessing __clinical usefulness__ requires other approaches and often requires insights from ___beyond the data___

---
## References and further reading

* [Steyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. Eur Heart J. 2014 Aug 1;35(29):1925â€“1931.](https://academic.oup.com/eurheartj/article/35/29/1925/2293109)

* Comparison with machine learning:<br/>[Breiman L. Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Stat Sci. Institute of Mathematical Statistics; 2001 Aug;16(3):199â€“231.](https://projecteuclid.org/euclid.ss/1009213726)

* [ROC curve animation by Dariya Sydykova](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation)

* [A Twitter thread on AUC by Cecile Janssens](https://twitter.com/cecilejanssens/status/1104134423673479169?lang=en)