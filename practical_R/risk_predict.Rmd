---
title: "Practical: RISK PREDICTION"
subtitle: "Advanced Statistics for Records Research"
output: pdf_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

## Research question

In this session, we will explore the dataset of 2000 participants we met in the lecture, and fit a risk prediction model for death within 5 years, based on some simple patient characteristics.

## Objectives

By the end of this practical, you should be able to:

1.  Fit a logistic model to create risk predictions.

2.  Assess model discrimination by calculating the Area Under the Curve.

3.  Assess model calibration by graphing observed and predicted risks.

## Dataset and analysis

For this practical we will use a (simulated) dataset called `data_predict`. This contains data for 2,000 patients, with information on six variables.

| Variable | Description                     |
|----------|---------------------------------|
| id       | Unique patient ID               |
| age      | Age (years)                     |
| sbp      | Systolic Blood Pressure (mm/Hg) |
| bmi      | Body Max Index $kg/m^2$         |
| sex      | Female / Male                   |
| dead     | Alive / Dead                    |

```{r setup}
# Install Libraries
if (!require(pacman)) install.packages("pacman")
pacman::p_load(tidyr, dplyr, ggplot2, broom, here, gtsummary)

# Load data
load(here::here("data/data_predict.rda"))
```

## Data exploration

Have a look at the data, then try answering the following participants: \* How many participants died at the end of the follow-up? \* What is the proportion of female participants? \* What is the mean, standard deviation, minimum, and maximum age of these participants?

```{r data}
# Base R
summary(data_predict)

# Tidyverse (More verbose but more control)
data_predict %>% 
  group_by(dead) %>% 
  tally %>% 
  mutate(percent = n/sum(n)*100)

data_predict %>% 
  group_by(sex) %>% 
  tally %>% 
  mutate(percent = n/sum(n)*100)

data_predict %>% 
  # filter out missing observation at any variable
  filter_all(all_vars(!is.na(.))) %>% 
  summarise(n = n(),
            mean = mean(age),
            sd = sd(age),
            min = min(age),
            max = max(age))

# or with gtsummary 
# https://www.danieldsjoberg.com/gtsummary/index.html
tbl_summary(data_predict)

```

About 25% of the 2,000 individuals die -- this is a high-risk population. There is roughly 50% males and 50% females, aged 40-80.

### Randomly split data into training and test sets

```{r partition}
set.seed(777)

n_total <- nrow(data_predict)
proportion_train <- 0.5
n_train <- floor(proportion_train * n_total)

sample_train <- sample(1:n_total, n_train) %>% sort
sample_test <- which(!(1:n_total %in% sample_train))

data_train <- data_predict[sample_train,]
data_test <- data_predict[sample_test,]
```

### Fit model in training data

```{r fit_model}
# Fit logistic regression
model <- glm(formula = dead ~ age + sex + sbp + bmi,
             family = "binomial",
             data = data_train)

# View model output summary
summary(model)

# Get odds ratio and 95% CI
cbind(OR = coef(model), confint(model)) %>% exp

# tidy method
tidy(model)
```

### Predict risk based on trained model

#### On training dataset

```{r predict_in_train}
# update data_train and data_test with predicted probabilites
data_train$prob_dead <- 
  predict(model, type = "response")

# compare predicted risk 
data_train %>% 
  group_by(dead) %>% 
  summarise(n = n (),
            mean = mean(prob_dead),
            sd = sd(prob_dead),
            min = min(prob_dead),
            max = max(prob_dead)) %>% 
  pivot_longer(-dead) %>% 
  pivot_wider(id_cols = name, names_from = dead)

```

#### On test dataset

```{r predict_in_test}
data_test$prob_dead <-
  predict(model, type = "response", newdata = data_test)

data_test %>% 
  group_by(dead) %>% 
  summarise(n = n (),
            mean = mean(prob_dead),
            sd = sd(prob_dead),
            min = min(prob_dead),
            max = max(prob_dead)) %>% 
  pivot_longer(-dead) %>% 
  pivot_wider(id_cols = name, names_from = dead)
```

```{r predict_alt}
# Alternative solution (with broom::augment() )
alt_data_train <- model %>%
  # augment creates new columns with some useful information from the model
  # .fitted = predicted values
  augment(type.predict = "response")

alt_data_test <- model %>%
  augment(type.predict = "response", newdata = data_test)

```

### Validation

#### ROC

In the training dataset, the ROC is 79%. This means that a person who did die has a 79% probability of having a higher predicted risk (of dying) than someone who did not. This shows the model has fairly good discrimination (ability to separate those who did and did not experience the event of interest).

```{r roc}
pacman::p_load(yardstick)

# combine dataset
data_grouped <- bind_rows(data_train %>% mutate(set = "Training"),
                      data_test %>% mutate(set = "Validation")) %>% 
  group_by(set)

# Calculate ROC
data_grouped %>% 
  # set the second level of factor variable as the event (i.e. dead)
  roc_auc(truth = dead, prob_dead, event_level = "second")

# Visualise ROC
data_roc <- data_grouped %>% 
  roc_curve(truth = dead, prob_dead, event_level = "second")

ggplot(data_roc, aes(x = 1-specificity, y = sensitivity, color = set)) +
  geom_abline(slope = 1, intercept = 0,
              size = 0.4, color = "grey21", linetype = "dashed") +
  geom_line() +
  theme_minimal(base_size = 12) +
  coord_equal()
  
```

#### Hosmer-Lemeshow

The Hosmer-Lemeshow goodness of fit table for the two (S=0 and the S=1) datasets were very similar. Both showed evidence of a well calibrated model.

```{r gof}
pacman::p_load(generalhoslem)

gof <- data_grouped %>% 
  group_map( ~ logitgof(
    obs = .$dead,
    exp = .$prob_dead,
    g = 10
  ))

# Fix name
names(gof) <- group_keys(data_grouped)[[group_vars(data_grouped)]]

# output Goodness of Fit metrics
gof

# create GoF table
gof_table <- lapply(gof, function(x){
  cbind(x$observed, x$expected) %>%
    as_tibble(rownames = "threshold") %>% 
    mutate(group = 1:nrow(.))
})

gof_table$Training
gof_table$Validation
```

Bar graphs comparing the predicted and observed risks in the S=0 and S=1 datasets also show good calibration (in both the training and validation data).

```{r gof_bar}
# tidy up gof_table for plotting
tidy_gof <- bind_rows(gof_table, .id = "set") %>%
  dplyr::select(set, group, Obs = y1, Exp = yhat1) %>% 
  pivot_longer(
    cols = c(Obs, Exp),
    names_to = "class"
  )

# plotting
tidy_gof %>% 
  ggplot(aes(x = group, y = value, fill = class)) +
  facet_grid(cols = vars(set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.title = element_blank())

```

### Calibration curve

```{r, fig.dim = c(6,4)}
df_model <- model %>%
  # augment creates new columns with some useful information from the model
  # .fitted = predicted values
  augment(type.predict = "link", newdata = data_test) %>%  
  mutate(dead_count = as.numeric(dead) - 1) %>% 
  arrange(.fitted)

# calculate alpha (calibration at large) & beta (calibration slope)
# alpha is calculated by constraining beta to 1

df_alpha <- glm(dead_count ~ offset(.fitted),
               data = df_model,
               family = binomial) %>% 
  tidy(conf.int = TRUE)

alpha_text <- paste0(round(df_alpha$estimate[1], 2),
                     " (95% CI: ", round(df_alpha$conf.low[1], 2),
                     " to ", round(df_alpha$conf.high[1], 2),
                     ")")

# beta is calculated by constraining alpha to 0
df_beta <- glm(dead_count ~ .fitted - 1,
                data = df_model,
                family = binomial) %>% 
  tidy(conf.int = TRUE)

beta_text <- paste0(round(df_beta$estimate[1], 2),
                    " (95% CI: ", round(df_beta$conf.low[1], 2),
                    " to ", round(df_beta$conf.high[1], 2),
                    ")")


# calculate mean predicted results per decile
df_sum_decile <- model %>%
  # augment creates new columns with some useful information from the model
  # .fitted = predicted values
  augment(type.predict = "response", newdata = data_test) %>%  
  mutate(dead_count = as.numeric(dead) - 1) %>% 
  arrange(.fitted) %>% 
  mutate(decile = ntile(.fitted, 10)) %>% 
  group_by(decile) %>% 
  summarize(mean_prediction = mean(.fitted), 
            sd_prediction = sd(.fitted),
            prop_observed = mean(dead_count)) %>% 
  mutate(dummy = "decile")

# plotting
ggplot(df_sum_decile, aes(x = mean_prediction, y = prop_observed)) +
  theme_minimal() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # geom_abline(slope = 1, intercept = df_alpha$estimate[1], color = "#1b5299") +
  geom_point(aes(color = dummy), size = 3) +
  scale_color_manual(values = "steelblue") +
  labs(x = "Mean Predicted", y = "Mean Observed", title = "Calibration plot") +
  annotate("label", x = -Inf, y = Inf, hjust = 0, vjust = 1.5, label.size = 0,
           label = paste("calibration-in-the-large =",
                         alpha_text)) +
  annotate("label", x = -Inf, y = Inf, hjust = 0, vjust = 2.5, label.size = 0,
           label = paste("calibration slope =",
                         beta_text)) +
  theme(legend.title = element_blank())

```

Since calibration-in-the-large is close to 0, and calibration slope is close to 1, we can say that the model is well calibrated
